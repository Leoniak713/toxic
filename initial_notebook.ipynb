{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import transformers\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.optim import *\n",
    "from torch.nn.modules.loss import *\n",
    "from torch.optim.lr_scheduler import * \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Seeds basic parameters for reproductibility of results\n",
    "    \n",
    "    Arguments:\n",
    "        seed {int} -- Number of the seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2020\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv(\"/home/maciej/Workspace/toxic/data/jigsaw-toxic-comment-train.csv\")\n",
    "train2 = pd.read_csv(\"/home/maciej/Workspace/toxic/data/jigsaw-unintended-bias-train.csv\")\n",
    "train2.toxic = train2.toxic.round().astype(int)\n",
    "\n",
    "valid = pd.read_csv('/home/maciej/Workspace/toxic/data/validation.csv')\n",
    "test = pd.read_csv('/home/maciej/Workspace/toxic/data/test.csv')\n",
    "sub = pd.read_csv('/home/maciej/Workspace/toxic/data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([\n",
    "    train1[['comment_text', 'toxic']],\n",
    "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train2[['comment_text', 'toxic']].query('toxic==0')#.sample(n=100000, random_state=0)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "    \"\"\"\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 192\n",
    "MODEL = 'xlm-roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 57s, sys: 3.51 s, total: 9min 1s\n",
      "Wall time: 9min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "x_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train.toxic.values\n",
    "y_valid = valid.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2125743, 192)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2125743,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Base class that defines common API for datasets.\"\"\"\n",
    "\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if self.y is not None:\n",
    "            return self.x[index], self.y[index]\n",
    "        return self.x[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, model, num_classes=1):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \n",
    "        Arguments:\n",
    "            model {string} -- Transformer to build the model on. Expects \"camembert-base\".\n",
    "            num_classes {int} -- Number of classes (default: {1})\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = model\n",
    "\n",
    "        self.nb_features = self.transformer.pooler.dense.out_features\n",
    "\n",
    "        self.pooler = nn.Sequential(\n",
    "            nn.Linear(self.nb_features, self.nb_features), \n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.logit = nn.Linear(self.nb_features, num_classes)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        \"\"\"\n",
    "        Usual torch forward function\n",
    "        \n",
    "        Arguments:\n",
    "            tokens {torch tensor} -- Sentence tokens\n",
    "        \n",
    "        Returns:\n",
    "            torch tensor -- Class logits\n",
    "        \"\"\"\n",
    "        hidden_states, _ = self.transformer(\n",
    "            tokens, attention_mask=(tokens > 0).long()\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states[:, 0] # Use the representation of the first token of the last layer\n",
    "\n",
    "        ft = self.pooler(hidden_states)\n",
    "\n",
    "        return self.logit(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_dataset, val_dataset, epochs=1, batch_size=32, warmup_prop=0, lr=5e-5):\n",
    "    device = 'cuda' #xm.xla_device()\n",
    "    model.to(device)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    num_warmup_steps = int(warmup_prop * epochs * len(train_loader))\n",
    "    num_training_steps = epochs * len(train_loader)\n",
    "    \n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    loss_fct = nn.BCEWithLogitsLoss(reduction='mean').to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        avg_loss = 0\n",
    "        \n",
    "        for step, (x, y_batch) in tqdm(enumerate(train_loader), total=len(train_loader)): \n",
    "            y_pred = model(x.to(device))\n",
    "            \n",
    "            loss = loss_fct(y_pred.view(-1).float(), y_batch.float().to(device))\n",
    "            loss.backward()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            print('{step}/{total}'.format(step=step, total=len(train_loader)))\n",
    "                \n",
    "        model.eval()\n",
    "        preds = []\n",
    "        truths = []\n",
    "        avg_val_loss = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y_batch in val_loader:                \n",
    "                y_pred = model(x.to(device))\n",
    "                loss = loss_fct(y_pred.detach().view(-1).float(), y_batch.float().to(device))\n",
    "                avg_val_loss += loss.item() / len(val_loader)\n",
    "                \n",
    "                probs = torch.sigmoid(y_pred).detach().cpu().numpy()\n",
    "                preds += list(probs.flatten())\n",
    "                truths += list(y_batch.numpy().flatten())\n",
    "            score = roc_auc_score(truths, preds)\n",
    "            \n",
    "        \n",
    "        dt = time.time() - start_time\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        print('Epoch {epoch}/{epochs} \\t lr={lr} \\t t={dt}s \\t loss={avg_loss} \\t val_loss={avg_val_loss} \\t val_auc={score}'.format(\n",
    "            epoch=epoch,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            dt=dt,\n",
    "            avg_loss=avg_loss,\n",
    "            avg_val_loss=avg_val_loss,\n",
    "            score=score\n",
    "        ))\n",
    "        #print(f'Epoch {epoch + 1}/{epochs} \\t lr={lr:.1e} \\t t={dt:.0f}s \\t loss={avg_loss:.4f} \\t val_loss={avg_val_loss:.4f} \\t val_auc={score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1 # 1 epoch seems to be enough\n",
    "batch_size = 16\n",
    "warmup_prop = 0.1\n",
    "lr = 2e-5  # Important parameter to tweak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer = AutoModel.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier = Transformer(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit(classifier, train_dataset, valid_dataset, epochs=epochs, batch_size=batch_size, warmup_prop=warmup_prop, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier.transformer.save_pretrained('model/first_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset, batch_size=16):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    truths = []\n",
    "    avg_val_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, x in tqdm(enumerate(loader), total=len(loader)):                \n",
    "            y_pred = model(x.to('cuda'))\n",
    "            probs = torch.sigmoid(y_pred).detach().cpu().numpy()\n",
    "            preds += list(probs.flatten())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = AutoModel.from_pretrained('./model/first_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Transformer(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (transformer): XLMRobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pooler): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (logit): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Doctor Who adlı viki başlığına 12. doctor olar...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Quindi tu sei uno di quelli   conservativi  , ...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Malesef gerçekleştirilmedi ancak şöyle bir şey...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>:Resim:Seldabagcan.jpg resminde kaynak sorunu ...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Le truc le plus important dans ta tirade c est...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>20px Caro editor, encontramos problemas na edi...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>el skate es unos de los deportes favoritos de ...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Me doy la bienvenida. A este usuari le gusta c...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>ES NOTABLEMENTE TENDENCIOSO, NO SE HABLA DE CU...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Merhaba. Düzelttiğin için teşekkürler. İngiliz...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>stacy é uma garoat cat cat cat que vai te sedu...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>C est surtout un sacré dévot et calotin idiot...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Le contributeur  y  tente de prouver par l abs...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Quer ofender, vai editar a desciclopédia (que ...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Já é a segunda ou terceira vez que insiste nes...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>==À propos de léon99 Le langage sauvage, le di...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Merhaba Abuk Sabuk, Ankara (şehir) maddesiyle ...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>, под вашу ответственность. NB: ЭСБЕ, значит, ...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Pour qui tu te prends? Comment oses-tu dire qu...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>Ошибаетесь Вы. Во-первых, текст должен быть п...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Mon cher Dibbouk, vous dites là une chose trè...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>El Jardín de infantes Nº938, fundado en 1989, ...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Mesmo ridículo, ainda para mais neste Mundo qu...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Por favor, deixe o seu e-mail na minha página ...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>PRIMEIRO ANTES DE CERTOS  CIDADÃOS  FICAREM FA...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>Отличная работа, спасибо. Я чуть доработал - п...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>50px|rightCaro editor, uma ou algumas de suas ...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>левакофашист и путинский лизун ??? Как вообще ...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>Caro André. Se pretende continuar editando, va...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63782</th>\n",
       "      <td>63782</td>\n",
       "      <td>Ciao, ho tradotto questa voce e vedo che in in...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63783</th>\n",
       "      <td>63783</td>\n",
       "      <td>Los mongoles emplearon una crueldad apabullant...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63784</th>\n",
       "      <td>63784</td>\n",
       "      <td>Par contre, supprimer un passage est considéré...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63785</th>\n",
       "      <td>63785</td>\n",
       "      <td>Reli e entendi, a essa hora querias o que? A q...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63786</th>\n",
       "      <td>63786</td>\n",
       "      <td>Qu’est-ce que c’est que ce bordel ? Wikipédia...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63787</th>\n",
       "      <td>63787</td>\n",
       "      <td>I N S T I - TI! T U - TU! T O - TO! INSTITUTO ...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63788</th>\n",
       "      <td>63788</td>\n",
       "      <td>Буш принимает психотропные Правда или грязная ...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63789</th>\n",
       "      <td>63789</td>\n",
       "      <td>Пластичность сексуальной ориентации в обе сто...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63790</th>\n",
       "      <td>63790</td>\n",
       "      <td>Источник не авторитетный. Написано под редакци...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63791</th>\n",
       "      <td>63791</td>\n",
       "      <td>Önemli değil. postyorum  Tam bilgi kutusu ekle...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63792</th>\n",
       "      <td>63792</td>\n",
       "      <td>Prezado Leon Saldanha, primeiramente agradeço ...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63793</th>\n",
       "      <td>63793</td>\n",
       "      <td>- Te rispondi di getto, dici anche cose sensat...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63794</th>\n",
       "      <td>63794</td>\n",
       "      <td>См. Великий марш возвращения → Столкновения на...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63795</th>\n",
       "      <td>63795</td>\n",
       "      <td>Ti chiedo cortesemente di non insistere con l ...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63796</th>\n",
       "      <td>63796</td>\n",
       "      <td>Encore un mauvais usage des références: la rev...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63797</th>\n",
       "      <td>63797</td>\n",
       "      <td>« sans oublier WP:CON vu que vous affirmez des...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63798</th>\n",
       "      <td>63798</td>\n",
       "      <td>Caro editor(a), bem-vindo(a) à Wikipédia. Enc...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63799</th>\n",
       "      <td>63799</td>\n",
       "      <td>Предлагаю добавить вот ещё что про коммерческу...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63800</th>\n",
       "      <td>63800</td>\n",
       "      <td>C’est très marrant , vous réagissez comme une ...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63801</th>\n",
       "      <td>63801</td>\n",
       "      <td>... получил сталинскую премию за музыку к филь...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63802</th>\n",
       "      <td>63802</td>\n",
       "      <td>Ckallogo.jpg lisans sorunu 64px|left|Dosya tel...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63803</th>\n",
       "      <td>63803</td>\n",
       "      <td>Разумеется Вам, как неспециалисту трудно вест...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63804</th>\n",
       "      <td>63804</td>\n",
       "      <td>Te agradezco tu cordial mensaje. De los cambio...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63805</th>\n",
       "      <td>63805</td>\n",
       "      <td>Amigo mio, quien diría que esta travesía de lo...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63806</th>\n",
       "      <td>63806</td>\n",
       "      <td>Evet haklısın telifli etiketi koymuşsun demişi...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63807</th>\n",
       "      <td>63807</td>\n",
       "      <td>No, non risponderò, come preannunciato. Prefer...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63808</th>\n",
       "      <td>63808</td>\n",
       "      <td>Ciao, I tecnici della Wikimedia Foundation sta...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63809</th>\n",
       "      <td>63809</td>\n",
       "      <td>innnazitutto ti ringrazio per i ringraziamenti...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63810</th>\n",
       "      <td>63810</td>\n",
       "      <td>Kaç olumlu oy gerekiyor? Şu an 7 oldu.  Hayır...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63811</th>\n",
       "      <td>63811</td>\n",
       "      <td>Te pido disculpas. La verdad es que no me per...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63812 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            content lang\n",
       "0          0  Doctor Who adlı viki başlığına 12. doctor olar...   tr\n",
       "1          1   Вполне возможно, но я пока не вижу необходимо...   ru\n",
       "2          2  Quindi tu sei uno di quelli   conservativi  , ...   it\n",
       "3          3  Malesef gerçekleştirilmedi ancak şöyle bir şey...   tr\n",
       "4          4  :Resim:Seldabagcan.jpg resminde kaynak sorunu ...   tr\n",
       "5          5  Le truc le plus important dans ta tirade c est...   fr\n",
       "6          6  20px Caro editor, encontramos problemas na edi...   pt\n",
       "7          7  el skate es unos de los deportes favoritos de ...   es\n",
       "8          8  Me doy la bienvenida. A este usuari le gusta c...   es\n",
       "9          9  ES NOTABLEMENTE TENDENCIOSO, NO SE HABLA DE CU...   es\n",
       "10        10  Merhaba. Düzelttiğin için teşekkürler. İngiliz...   tr\n",
       "11        11  stacy é uma garoat cat cat cat que vai te sedu...   pt\n",
       "12        12   C est surtout un sacré dévot et calotin idiot...   fr\n",
       "13        13  Le contributeur  y  tente de prouver par l abs...   fr\n",
       "14        14  Quer ofender, vai editar a desciclopédia (que ...   pt\n",
       "15        15  Já é a segunda ou terceira vez que insiste nes...   pt\n",
       "16        16  ==À propos de léon99 Le langage sauvage, le di...   fr\n",
       "17        17  Merhaba Abuk Sabuk, Ankara (şehir) maddesiyle ...   tr\n",
       "18        18  , под вашу ответственность. NB: ЭСБЕ, значит, ...   ru\n",
       "19        19  Pour qui tu te prends? Comment oses-tu dire qu...   fr\n",
       "20        20   Ошибаетесь Вы. Во-первых, текст должен быть п...   ru\n",
       "21        21   Mon cher Dibbouk, vous dites là une chose trè...   fr\n",
       "22        22  El Jardín de infantes Nº938, fundado en 1989, ...   es\n",
       "23        23  Mesmo ridículo, ainda para mais neste Mundo qu...   pt\n",
       "24        24  Por favor, deixe o seu e-mail na minha página ...   pt\n",
       "25        25  PRIMEIRO ANTES DE CERTOS  CIDADÃOS  FICAREM FA...   pt\n",
       "26        26  Отличная работа, спасибо. Я чуть доработал - п...   ru\n",
       "27        27  50px|rightCaro editor, uma ou algumas de suas ...   pt\n",
       "28        28  левакофашист и путинский лизун ??? Как вообще ...   ru\n",
       "29        29  Caro André. Se pretende continuar editando, va...   pt\n",
       "...      ...                                                ...  ...\n",
       "63782  63782  Ciao, ho tradotto questa voce e vedo che in in...   it\n",
       "63783  63783  Los mongoles emplearon una crueldad apabullant...   es\n",
       "63784  63784  Par contre, supprimer un passage est considéré...   fr\n",
       "63785  63785  Reli e entendi, a essa hora querias o que? A q...   pt\n",
       "63786  63786   Qu’est-ce que c’est que ce bordel ? Wikipédia...   fr\n",
       "63787  63787  I N S T I - TI! T U - TU! T O - TO! INSTITUTO ...   es\n",
       "63788  63788  Буш принимает психотропные Правда или грязная ...   ru\n",
       "63789  63789   Пластичность сексуальной ориентации в обе сто...   ru\n",
       "63790  63790  Источник не авторитетный. Написано под редакци...   ru\n",
       "63791  63791  Önemli değil. postyorum  Tam bilgi kutusu ekle...   tr\n",
       "63792  63792  Prezado Leon Saldanha, primeiramente agradeço ...   pt\n",
       "63793  63793  - Te rispondi di getto, dici anche cose sensat...   it\n",
       "63794  63794  См. Великий марш возвращения → Столкновения на...   ru\n",
       "63795  63795  Ti chiedo cortesemente di non insistere con l ...   it\n",
       "63796  63796  Encore un mauvais usage des références: la rev...   fr\n",
       "63797  63797  « sans oublier WP:CON vu que vous affirmez des...   fr\n",
       "63798  63798   Caro editor(a), bem-vindo(a) à Wikipédia. Enc...   pt\n",
       "63799  63799  Предлагаю добавить вот ещё что про коммерческу...   ru\n",
       "63800  63800  C’est très marrant , vous réagissez comme une ...   fr\n",
       "63801  63801  ... получил сталинскую премию за музыку к филь...   ru\n",
       "63802  63802  Ckallogo.jpg lisans sorunu 64px|left|Dosya tel...   tr\n",
       "63803  63803   Разумеется Вам, как неспециалисту трудно вест...   ru\n",
       "63804  63804  Te agradezco tu cordial mensaje. De los cambio...   es\n",
       "63805  63805  Amigo mio, quien diría que esta travesía de lo...   es\n",
       "63806  63806  Evet haklısın telifli etiketi koymuşsun demişi...   tr\n",
       "63807  63807  No, non risponderò, come preannunciato. Prefer...   it\n",
       "63808  63808  Ciao, I tecnici della Wikimedia Foundation sta...   it\n",
       "63809  63809  innnazitutto ti ringrazio per i ringraziamenti...   it\n",
       "63810  63810   Kaç olumlu oy gerekiyor? Şu an 7 oldu.  Hayır...   tr\n",
       "63811  63811   Te pido disculpas. La verdad es que no me per...   es\n",
       "\n",
       "[63812 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef3d534ed1642f98f2882d139c96ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3989.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds = predict(classifier, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0e707be41e40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Workspace/toxic/env_toxic/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-11cc56661f97>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \"\"\"\n\u001b[1;32m     33\u001b[0m         hidden_states, _ = self.transformer(\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'long'"
     ]
    }
   ],
   "source": [
    "classifier(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63812"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['toxic'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.041890193,\n",
       " 0.04364335,\n",
       " 0.061732296,\n",
       " 0.036378905,\n",
       " 0.037885625,\n",
       " 0.046357226,\n",
       " 0.04339461,\n",
       " 0.037015777,\n",
       " 0.35728034,\n",
       " 0.17888205,\n",
       " 0.036505077,\n",
       " 0.20173225,\n",
       " 0.82753825,\n",
       " 0.049571786,\n",
       " 0.29557827,\n",
       " 0.063967206,\n",
       " 0.038357083,\n",
       " 0.03453088,\n",
       " 0.03396784,\n",
       " 0.07432015,\n",
       " 0.046146777,\n",
       " 0.03714179,\n",
       " 0.043859903,\n",
       " 0.29813716,\n",
       " 0.036270916,\n",
       " 0.14713489,\n",
       " 0.04335147,\n",
       " 0.036112223,\n",
       " 0.29121426,\n",
       " 0.041792236,\n",
       " 0.123881586,\n",
       " 0.037975606,\n",
       " 0.066351995,\n",
       " 0.040454026,\n",
       " 0.040251948,\n",
       " 0.043759555,\n",
       " 0.044165492,\n",
       " 0.03577595,\n",
       " 0.13794467,\n",
       " 0.039145913,\n",
       " 0.037136607,\n",
       " 0.036153287,\n",
       " 0.14240599,\n",
       " 0.05101105,\n",
       " 0.041368492,\n",
       " 0.037966937,\n",
       " 0.050588634,\n",
       " 0.043965537,\n",
       " 0.046192583,\n",
       " 0.07301474,\n",
       " 0.31763434,\n",
       " 0.04152216,\n",
       " 0.36518833,\n",
       " 0.035191048,\n",
       " 0.041350797,\n",
       " 0.0458616,\n",
       " 0.0439705,\n",
       " 0.744836,\n",
       " 0.03986522,\n",
       " 0.06365075,\n",
       " 0.25605536,\n",
       " 0.04288899,\n",
       " 0.08948533,\n",
       " 0.049614873,\n",
       " 0.31162903,\n",
       " 0.041092165,\n",
       " 0.0438178,\n",
       " 0.04448736,\n",
       " 0.04199287,\n",
       " 0.09095582,\n",
       " 0.21469893,\n",
       " 0.03612182,\n",
       " 0.30643794,\n",
       " 0.037645206,\n",
       " 0.039536543,\n",
       " 0.036856078,\n",
       " 0.038427956,\n",
       " 0.03774337,\n",
       " 0.23433055,\n",
       " 0.07629361,\n",
       " 0.083599314,\n",
       " 0.045425314,\n",
       " 0.050723147,\n",
       " 0.04232924,\n",
       " 0.39475587,\n",
       " 0.038360193,\n",
       " 0.037443284,\n",
       " 0.043572113,\n",
       " 0.043671828,\n",
       " 0.2492151,\n",
       " 0.035572827,\n",
       " 0.09145587,\n",
       " 0.04333898,\n",
       " 0.0451542,\n",
       " 0.042386774,\n",
       " 0.04339286,\n",
       " 0.14732161,\n",
       " 0.3259541,\n",
       " 0.0436743,\n",
       " 0.036000755,\n",
       " 0.042841718,\n",
       " 0.08112696,\n",
       " 0.034589984,\n",
       " 0.041344136,\n",
       " 0.16927284,\n",
       " 0.03824305,\n",
       " 0.039273906,\n",
       " 0.034896534,\n",
       " 0.055424172,\n",
       " 0.033835687,\n",
       " 0.042500637,\n",
       " 0.088082545,\n",
       " 0.03411179,\n",
       " 0.09362834,\n",
       " 0.039886203,\n",
       " 0.16339205,\n",
       " 0.036280178,\n",
       " 0.03628163,\n",
       " 0.03710101,\n",
       " 0.044210777,\n",
       " 0.03901146,\n",
       " 0.04389244,\n",
       " 0.04122163,\n",
       " 0.03870535,\n",
       " 0.07496456,\n",
       " 0.2216334,\n",
       " 0.04689459,\n",
       " 0.05142492,\n",
       " 0.060162704,\n",
       " 0.06489456,\n",
       " 0.036950085,\n",
       " 0.050220948,\n",
       " 0.044576205,\n",
       " 0.039877597,\n",
       " 0.04034558,\n",
       " 0.042870227,\n",
       " 0.042874333,\n",
       " 0.2978261,\n",
       " 0.08220078,\n",
       " 0.12379242,\n",
       " 0.34455577,\n",
       " 0.044556797,\n",
       " 0.12880656,\n",
       " 0.043474354,\n",
       " 0.03781302,\n",
       " 0.05607966,\n",
       " 0.18079324,\n",
       " 0.03823151,\n",
       " 0.0367354,\n",
       " 0.041718505,\n",
       " 0.14272898,\n",
       " 0.83153653,\n",
       " 0.056506354,\n",
       " 0.040176336,\n",
       " 0.12920506,\n",
       " 0.05342344,\n",
       " 0.042347837,\n",
       " 0.044076636,\n",
       " 0.043877963,\n",
       " 0.047125284,\n",
       " 0.13016121,\n",
       " 0.04505544,\n",
       " 0.04062266,\n",
       " 0.03602002,\n",
       " 0.043821607,\n",
       " 0.04072416,\n",
       " 0.13234445,\n",
       " 0.037625924,\n",
       " 0.04319095,\n",
       " 0.04480655,\n",
       " 0.042667482,\n",
       " 0.04885297,\n",
       " 0.0378469,\n",
       " 0.040885814,\n",
       " 0.06852448,\n",
       " 0.040149994,\n",
       " 0.03466464,\n",
       " 0.037190497,\n",
       " 0.0435022,\n",
       " 0.046967264,\n",
       " 0.04876884,\n",
       " 0.05465901,\n",
       " 0.035097063,\n",
       " 0.03778637,\n",
       " 0.04392589,\n",
       " 0.23763159,\n",
       " 0.03523147,\n",
       " 0.16501565,\n",
       " 0.04370822,\n",
       " 0.03862675,\n",
       " 0.043651305,\n",
       " 0.036855407,\n",
       " 0.10533277,\n",
       " 0.035534296,\n",
       " 0.052631933,\n",
       " 0.042497467,\n",
       " 0.04183458,\n",
       " 0.12978607,\n",
       " 0.037278313,\n",
       " 0.7730987,\n",
       " 0.0431626,\n",
       " 0.036828287,\n",
       " 0.042619027,\n",
       " 0.045009762,\n",
       " 0.039885625,\n",
       " 0.04493012,\n",
       " 0.03976808,\n",
       " 0.37322313,\n",
       " 0.15292658,\n",
       " 0.042900443,\n",
       " 0.043470915,\n",
       " 0.051796537,\n",
       " 0.038758993,\n",
       " 0.16461489,\n",
       " 0.03540809,\n",
       " 0.17101885,\n",
       " 0.39316767,\n",
       " 0.16926026,\n",
       " 0.13882227,\n",
       " 0.28564537,\n",
       " 0.04740242,\n",
       " 0.045327213,\n",
       " 0.25946835,\n",
       " 0.057060566,\n",
       " 0.042754095,\n",
       " 0.107906066,\n",
       " 0.04291048,\n",
       " 0.058896642,\n",
       " 0.05566594,\n",
       " 0.20226407,\n",
       " 0.04502782,\n",
       " 0.041977,\n",
       " 0.2293241,\n",
       " 0.052431185,\n",
       " 0.044781465,\n",
       " 0.03570512,\n",
       " 0.053247225,\n",
       " 0.038928136,\n",
       " 0.06840217,\n",
       " 0.05820602,\n",
       " 0.043717608,\n",
       " 0.04575424,\n",
       " 0.044031765,\n",
       " 0.043977577,\n",
       " 0.061221205,\n",
       " 0.053420074,\n",
       " 0.037946682,\n",
       " 0.046578765,\n",
       " 0.08395382,\n",
       " 0.15731807,\n",
       " 0.03739013,\n",
       " 0.52816015,\n",
       " 0.03792064,\n",
       " 0.03491171,\n",
       " 0.034482587,\n",
       " 0.03731996,\n",
       " 0.04501122,\n",
       " 0.044323206,\n",
       " 0.04586539,\n",
       " 0.03594143,\n",
       " 0.03957196,\n",
       " 0.04136941,\n",
       " 0.053480122,\n",
       " 0.047186866,\n",
       " 0.040162627,\n",
       " 0.056050975,\n",
       " 0.0396245,\n",
       " 0.04394503,\n",
       " 0.03824813,\n",
       " 0.039317343,\n",
       " 0.04172822,\n",
       " 0.041502707,\n",
       " 0.037286628,\n",
       " 0.044637598,\n",
       " 0.03385818,\n",
       " 0.19691065,\n",
       " 0.4194012,\n",
       " 0.04499931,\n",
       " 0.038339183,\n",
       " 0.03831236,\n",
       " 0.044339363,\n",
       " 0.17999998,\n",
       " 0.043315135,\n",
       " 0.08045236,\n",
       " 0.050212156,\n",
       " 0.038836144,\n",
       " 0.04624175,\n",
       " 0.08696583,\n",
       " 0.049427148,\n",
       " 0.063150935,\n",
       " 0.04391567,\n",
       " 0.039706778,\n",
       " 0.03801018,\n",
       " 0.03615713,\n",
       " 0.03320638,\n",
       " 0.03920079,\n",
       " 0.04373632,\n",
       " 0.038954563,\n",
       " 0.03474257,\n",
       " 0.039116837,\n",
       " 0.040762793,\n",
       " 0.043631967,\n",
       " 0.049624477,\n",
       " 0.03757329,\n",
       " 0.046671495,\n",
       " 0.036010236,\n",
       " 0.03968685,\n",
       " 0.04673133,\n",
       " 0.04304498,\n",
       " 0.04103385,\n",
       " 0.040373456,\n",
       " 0.05494714,\n",
       " 0.034927357,\n",
       " 0.036772676,\n",
       " 0.14049761,\n",
       " 0.034206524,\n",
       " 0.043485776,\n",
       " 0.044031363,\n",
       " 0.04114047,\n",
       " 0.04337157,\n",
       " 0.043866705,\n",
       " 0.03979852,\n",
       " 0.035126723,\n",
       " 0.043853205,\n",
       " 0.11122794,\n",
       " 0.03437149,\n",
       " 0.038611744,\n",
       " 0.03960918,\n",
       " 0.0344983,\n",
       " 0.082752086,\n",
       " 0.04060899,\n",
       " 0.05208982,\n",
       " 0.0482116,\n",
       " 0.043829706,\n",
       " 0.1060366,\n",
       " 0.03828917,\n",
       " 0.035411935,\n",
       " 0.08249342,\n",
       " 0.057742532,\n",
       " 0.057858292,\n",
       " 0.048341468,\n",
       " 0.04072868,\n",
       " 0.15916465,\n",
       " 0.41404572,\n",
       " 0.043638434,\n",
       " 0.043979794,\n",
       " 0.06425014,\n",
       " 0.038846727,\n",
       " 0.038077172,\n",
       " 0.05648141,\n",
       " 0.043201376,\n",
       " 0.044281192,\n",
       " 0.045088593,\n",
       " 0.03488063,\n",
       " 0.1722239,\n",
       " 0.04422852,\n",
       " 0.042663652,\n",
       " 0.088872984,\n",
       " 0.043889455,\n",
       " 0.10274448,\n",
       " 0.054071195,\n",
       " 0.04234066,\n",
       " 0.04367378,\n",
       " 0.041889004,\n",
       " 0.15416418,\n",
       " 0.043737873,\n",
       " 0.035652876,\n",
       " 0.037175935,\n",
       " 0.26876047,\n",
       " 0.034869213,\n",
       " 0.037515376,\n",
       " 0.6342136,\n",
       " 0.10239292,\n",
       " 0.043905355,\n",
       " 0.3471109,\n",
       " 0.38637647,\n",
       " 0.09066973,\n",
       " 0.041401897,\n",
       " 0.03784749,\n",
       " 0.033736758,\n",
       " 0.06605174,\n",
       " 0.14045109,\n",
       " 0.06130524,\n",
       " 0.043853227,\n",
       " 0.037580356,\n",
       " 0.040067967,\n",
       " 0.04200158,\n",
       " 0.31042677,\n",
       " 0.03488669,\n",
       " 0.036564074,\n",
       " 0.041348014,\n",
       " 0.035536632,\n",
       " 0.03479389,\n",
       " 0.03868286,\n",
       " 0.05258134,\n",
       " 0.04579424,\n",
       " 0.03825448,\n",
       " 0.04315311,\n",
       " 0.047859047,\n",
       " 0.041163746,\n",
       " 0.035973158,\n",
       " 0.3021817,\n",
       " 0.04353494,\n",
       " 0.042562496,\n",
       " 0.10335298,\n",
       " 0.18345541,\n",
       " 0.045276515,\n",
       " 0.08564529,\n",
       " 0.16555339,\n",
       " 0.06774098,\n",
       " 0.04041866,\n",
       " 0.040531106,\n",
       " 0.12914622,\n",
       " 0.038444594,\n",
       " 0.20566538,\n",
       " 0.048183877,\n",
       " 0.04026668,\n",
       " 0.079821475,\n",
       " 0.039691675,\n",
       " 0.03473533,\n",
       " 0.03902418,\n",
       " 0.042562496,\n",
       " 0.036349244,\n",
       " 0.04384222,\n",
       " 0.08551907,\n",
       " 0.03918946,\n",
       " 0.18393508,\n",
       " 0.039206937,\n",
       " 0.10547069,\n",
       " 0.25322816,\n",
       " 0.040545832,\n",
       " 0.05418752,\n",
       " 0.04324594,\n",
       " 0.038246173,\n",
       " 0.12246827,\n",
       " 0.052596234,\n",
       " 0.039452687,\n",
       " 0.034171656,\n",
       " 0.039528698,\n",
       " 0.046288416,\n",
       " 0.04734107,\n",
       " 0.040095627,\n",
       " 0.043481383,\n",
       " 0.048803102,\n",
       " 0.25452626,\n",
       " 0.04074029,\n",
       " 0.07864438,\n",
       " 0.043201387,\n",
       " 0.042769857,\n",
       " 0.43615085,\n",
       " 0.034314908,\n",
       " 0.59263355,\n",
       " 0.20524633,\n",
       " 0.06489408,\n",
       " 0.13584165,\n",
       " 0.26791373,\n",
       " 0.041195456,\n",
       " 0.04331879,\n",
       " 0.46806788,\n",
       " 0.077899694,\n",
       " 0.048717603,\n",
       " 0.04055913,\n",
       " 0.35665485,\n",
       " 0.10508385,\n",
       " 0.038563408,\n",
       " 0.039140407,\n",
       " 0.3231793,\n",
       " 0.18867394,\n",
       " 0.03999895,\n",
       " 0.043773327,\n",
       " 0.038406108,\n",
       " 0.03486901,\n",
       " 0.05357643,\n",
       " 0.041154575,\n",
       " 0.046371184,\n",
       " 0.07463499,\n",
       " 0.04514259,\n",
       " 0.041004617,\n",
       " 0.4072354,\n",
       " 0.04523858,\n",
       " 0.039865103,\n",
       " 0.06254606,\n",
       " 0.049052548,\n",
       " 0.040344056,\n",
       " 0.048198257,\n",
       " 0.03493705,\n",
       " 0.0386834,\n",
       " 0.040369686,\n",
       " 0.045800272,\n",
       " 0.042659972,\n",
       " 0.03925405,\n",
       " 0.034329765,\n",
       " 0.036778715,\n",
       " 0.05637877,\n",
       " 0.30476707,\n",
       " 0.040071655,\n",
       " 0.09913182,\n",
       " 0.046702284,\n",
       " 0.038205326,\n",
       " 0.063706845,\n",
       " 0.03470032,\n",
       " 0.036373373,\n",
       " 0.1280904,\n",
       " 0.04971312,\n",
       " 0.04106435,\n",
       " 0.043766394,\n",
       " 0.034027234,\n",
       " 0.048183907,\n",
       " 0.037941795,\n",
       " 0.037556317,\n",
       " 0.044053156,\n",
       " 0.134447,\n",
       " 0.043893557,\n",
       " 0.043789,\n",
       " 0.03484477,\n",
       " 0.037834667,\n",
       " 0.041662343,\n",
       " 0.08821611,\n",
       " 0.05238879,\n",
       " 0.04391597,\n",
       " 0.049870268,\n",
       " 0.04412663,\n",
       " 0.21903673,\n",
       " 0.3398985,\n",
       " 0.05823575,\n",
       " 0.043428697,\n",
       " 0.04186795,\n",
       " 0.043611,\n",
       " 0.328492,\n",
       " 0.24111526,\n",
       " 0.039150666,\n",
       " 0.05126147,\n",
       " 0.036721423,\n",
       " 0.037689228,\n",
       " 0.29994497,\n",
       " 0.049318954,\n",
       " 0.03975767,\n",
       " 0.14422092,\n",
       " 0.22559346,\n",
       " 0.04347371,\n",
       " 0.06679965,\n",
       " 0.15433913,\n",
       " 0.039153215,\n",
       " 0.0507516,\n",
       " 0.039709244,\n",
       " 0.3465955,\n",
       " 0.04034461,\n",
       " 0.074238874,\n",
       " 0.04369841,\n",
       " 0.2484709,\n",
       " 0.06941231,\n",
       " 0.046392106,\n",
       " 0.05493531,\n",
       " 0.06823104,\n",
       " 0.0415357,\n",
       " 0.061894603,\n",
       " 0.0384576,\n",
       " 0.047933105,\n",
       " 0.9266473,\n",
       " 0.26486152,\n",
       " 0.042885575,\n",
       " 0.034433734,\n",
       " 0.043667536,\n",
       " 0.03611504,\n",
       " 0.039409246,\n",
       " 0.5281695,\n",
       " 0.16487071,\n",
       " 0.21138804,\n",
       " 0.034950707,\n",
       " 0.2629026,\n",
       " 0.039637428,\n",
       " 0.231894,\n",
       " 0.056933645,\n",
       " 0.03398427,\n",
       " 0.42550477,\n",
       " 0.041559365,\n",
       " 0.0456559,\n",
       " 0.1413947,\n",
       " 0.042098083,\n",
       " 0.21105613,\n",
       " 0.04353201,\n",
       " 0.062469468,\n",
       " 0.035284452,\n",
       " 0.10735456,\n",
       " 0.036502082,\n",
       " 0.12257288,\n",
       " 0.11247328,\n",
       " 0.067404866,\n",
       " 0.10360061,\n",
       " 0.034414474,\n",
       " 0.03810229,\n",
       " 0.048520558,\n",
       " 0.038395066,\n",
       " 0.04397163,\n",
       " 0.07095247,\n",
       " 0.040939618,\n",
       " 0.039905313,\n",
       " 0.050545864,\n",
       " 0.061386574,\n",
       " 0.0955129,\n",
       " 0.6421187,\n",
       " 0.037890952,\n",
       " 0.08860558,\n",
       " 0.12759721,\n",
       " 0.049121466,\n",
       " 0.14845194,\n",
       " 0.058432657,\n",
       " 0.034353826,\n",
       " 0.113159515,\n",
       " 0.037670374,\n",
       " 0.043974064,\n",
       " 0.09925302,\n",
       " 0.046645526,\n",
       " 0.04366987,\n",
       " 0.03481684,\n",
       " 0.18700162,\n",
       " 0.1778669,\n",
       " 0.32201904,\n",
       " 0.04455901,\n",
       " 0.04393481,\n",
       " 0.04434855,\n",
       " 0.038936306,\n",
       " 0.035862233,\n",
       " 0.057833057,\n",
       " 0.33444348,\n",
       " 0.040060487,\n",
       " 0.036578227,\n",
       " 0.09104902,\n",
       " 0.041380394,\n",
       " 0.2634698,\n",
       " 0.19902061,\n",
       " 0.1077758,\n",
       " 0.04548047,\n",
       " 0.35601646,\n",
       " 0.039501347,\n",
       " 0.059207343,\n",
       " 0.03466404,\n",
       " 0.041698225,\n",
       " 0.04954416,\n",
       " 0.035269395,\n",
       " 0.035577662,\n",
       " 0.044012822,\n",
       " 0.043459225,\n",
       " 0.038258146,\n",
       " 0.049408656,\n",
       " 0.037840433,\n",
       " 0.05706144,\n",
       " 0.041036624,\n",
       " 0.039901935,\n",
       " 0.10713455,\n",
       " 0.035660706,\n",
       " 0.044744298,\n",
       " 0.042562496,\n",
       " 0.03909662,\n",
       " 0.33151585,\n",
       " 0.16366935,\n",
       " 0.04140256,\n",
       " 0.036989048,\n",
       " 0.044279255,\n",
       " 0.22745511,\n",
       " 0.054724105,\n",
       " 0.052424375,\n",
       " 0.045181297,\n",
       " 0.23248285,\n",
       " 0.04433589,\n",
       " 0.03489081,\n",
       " 0.041121386,\n",
       " 0.03460761,\n",
       " 0.12780225,\n",
       " 0.08808146,\n",
       " 0.04298078,\n",
       " 0.03456869,\n",
       " 0.036284328,\n",
       " 0.043589666,\n",
       " 0.11322333,\n",
       " 0.038801607,\n",
       " 0.039443307,\n",
       " 0.03850395,\n",
       " 0.045108676,\n",
       " 0.0418927,\n",
       " 0.12396294,\n",
       " 0.040108174,\n",
       " 0.043526016,\n",
       " 0.034809373,\n",
       " 0.04136597,\n",
       " 0.096998185,\n",
       " 0.040092092,\n",
       " 0.057743657,\n",
       " 0.03903693,\n",
       " 0.06295027,\n",
       " 0.11201671,\n",
       " 0.043282546,\n",
       " 0.03446879,\n",
       " 0.042598166,\n",
       " 0.04005527,\n",
       " 0.46648312,\n",
       " 0.043721903,\n",
       " 0.07127044,\n",
       " 0.034667317,\n",
       " 0.03656252,\n",
       " 0.21780264,\n",
       " 0.044097207,\n",
       " 0.07066466,\n",
       " 0.03526735,\n",
       " 0.10641129,\n",
       " 0.0407531,\n",
       " 0.038107283,\n",
       " 0.042507004,\n",
       " 0.046861302,\n",
       " 0.043786824,\n",
       " 0.034814082,\n",
       " 0.07271534,\n",
       " 0.21487825,\n",
       " 0.049212687,\n",
       " 0.033576723,\n",
       " 0.03963372,\n",
       " 0.040804867,\n",
       " 0.044821735,\n",
       " 0.041381028,\n",
       " 0.03395498,\n",
       " 0.040900897,\n",
       " 0.043896414,\n",
       " 0.047146067,\n",
       " 0.039693203,\n",
       " 0.043549582,\n",
       " 0.06510206,\n",
       " 0.040373825,\n",
       " 0.45729,\n",
       " 0.051832,\n",
       " 0.039183926,\n",
       " 0.1474403,\n",
       " 0.09586241,\n",
       " 0.044270903,\n",
       " 0.5206851,\n",
       " 0.043832675,\n",
       " 0.037149515,\n",
       " 0.041883513,\n",
       " 0.043570943,\n",
       " 0.055407852,\n",
       " 0.061297305,\n",
       " 0.043796998,\n",
       " 0.050131638,\n",
       " 0.05345607,\n",
       " 0.03587452,\n",
       " 0.19025497,\n",
       " 0.09139228,\n",
       " 0.04077252,\n",
       " 0.0399261,\n",
       " 0.08054979,\n",
       " 0.054986354,\n",
       " 0.03559781,\n",
       " 0.10331977,\n",
       " 0.3529336,\n",
       " 0.038174454,\n",
       " 0.04334535,\n",
       " 0.043608505,\n",
       " 0.05564345,\n",
       " 0.034084503,\n",
       " 0.1812576,\n",
       " 0.051836945,\n",
       " 0.035317037,\n",
       " 0.041600283,\n",
       " 0.044044174,\n",
       " 0.035425227,\n",
       " 0.073479414,\n",
       " 0.30771047,\n",
       " 0.69025224,\n",
       " 0.049026664,\n",
       " 0.036445122,\n",
       " 0.034529828,\n",
       " 0.0662548,\n",
       " 0.3252021,\n",
       " 0.042003155,\n",
       " 0.044013195,\n",
       " 0.06748008,\n",
       " 0.04733755,\n",
       " 0.2816709,\n",
       " 0.12301739,\n",
       " 0.043508034,\n",
       " 0.049204435,\n",
       " 0.39810517,\n",
       " 0.046218634,\n",
       " 0.05913558,\n",
       " 0.112783045,\n",
       " 0.042860862,\n",
       " 0.034436885,\n",
       " 0.20165762,\n",
       " 0.045679875,\n",
       " 0.06642725,\n",
       " 0.035770234,\n",
       " 0.050594334,\n",
       " 0.038597904,\n",
       " 0.043923706,\n",
       " 0.27784678,\n",
       " 0.047074545,\n",
       " 0.14417268,\n",
       " 0.04381831,\n",
       " 0.058101565,\n",
       " 0.21528824,\n",
       " 0.07954226,\n",
       " 0.05387359,\n",
       " 0.04012559,\n",
       " 0.23007591,\n",
       " 0.06235935,\n",
       " 0.60936475,\n",
       " 0.04206403,\n",
       " 0.039094947,\n",
       " 0.048567098,\n",
       " 0.03532608,\n",
       " 0.24838167,\n",
       " 0.059885755,\n",
       " 0.034313723,\n",
       " 0.063076526,\n",
       " 0.039671563,\n",
       " 0.048866432,\n",
       " 0.035684746,\n",
       " 0.034796935,\n",
       " 0.10088172,\n",
       " 0.06363053,\n",
       " 0.049900863,\n",
       " 0.06950476,\n",
       " 0.043456808,\n",
       " 0.15327926,\n",
       " 0.13510919,\n",
       " 0.40601483,\n",
       " 0.0355261,\n",
       " 0.037588153,\n",
       " 0.1289127,\n",
       " 0.0420849,\n",
       " 0.043474987,\n",
       " 0.040363718,\n",
       " 0.3164506,\n",
       " 0.12523657,\n",
       " 0.041187964,\n",
       " 0.0859987,\n",
       " 0.042595472,\n",
       " 0.23812152,\n",
       " 0.03396361,\n",
       " 0.03477085,\n",
       " 0.038919516,\n",
       " 0.038648322,\n",
       " 0.04447656,\n",
       " 0.045075748,\n",
       " 0.039805744,\n",
       " 0.048705395,\n",
       " 0.03646432,\n",
       " 0.040342163,\n",
       " 0.034557234,\n",
       " 0.048188426,\n",
       " 0.03507598,\n",
       " 0.050772186,\n",
       " 0.043933928,\n",
       " 0.07559203,\n",
       " 0.063876994,\n",
       " 0.104724474,\n",
       " 0.14934275,\n",
       " 0.33555645,\n",
       " 0.03983149,\n",
       " 0.03692825,\n",
       " 0.30382428,\n",
       " 0.06144873,\n",
       " 0.038314633,\n",
       " 0.04950725,\n",
       " 0.036765356,\n",
       " 0.043671858,\n",
       " 0.0799691,\n",
       " 0.34817934,\n",
       " 0.0639762,\n",
       " 0.044778455,\n",
       " 0.22229223,\n",
       " 0.034576315,\n",
       " 0.341955,\n",
       " 0.20383948,\n",
       " 0.037750207,\n",
       " 0.11891332,\n",
       " 0.11380522,\n",
       " 0.3044177,\n",
       " 0.03811611,\n",
       " 0.07921594,\n",
       " 0.044390686,\n",
       " 0.04367381,\n",
       " 0.03671214,\n",
       " 0.11060904,\n",
       " 0.038863882,\n",
       " 0.043714236,\n",
       " 0.044995777,\n",
       " 0.03512357,\n",
       " 0.044564936,\n",
       " 0.05095146,\n",
       " 0.036173653,\n",
       " 0.053031262,\n",
       " 0.053389773,\n",
       " 0.13416876,\n",
       " 0.49868655,\n",
       " 0.06155453,\n",
       " 0.10147539,\n",
       " 0.3477978,\n",
       " 0.049011156,\n",
       " 0.060365584,\n",
       " 0.039628875,\n",
       " 0.040908776,\n",
       " 0.038109694,\n",
       " 0.18564266,\n",
       " 0.042157028,\n",
       " 0.059302546,\n",
       " 0.034729443,\n",
       " 0.043673493,\n",
       " 0.034970377,\n",
       " 0.26681918,\n",
       " 0.041523345,\n",
       " 0.03954667,\n",
       " 0.041445155,\n",
       " 0.043975078,\n",
       " 0.038850304,\n",
       " 0.043366075,\n",
       " 0.21183217,\n",
       " 0.045469295,\n",
       " 0.04048003,\n",
       " 0.45226386,\n",
       " 0.04568349,\n",
       " 0.03394807,\n",
       " 0.037399776,\n",
       " 0.05392352,\n",
       " 0.047593113,\n",
       " 0.034351766,\n",
       " 0.35044137,\n",
       " 0.042177267,\n",
       " 0.047534116,\n",
       " 0.038115006,\n",
       " 0.038283437,\n",
       " 0.4169925,\n",
       " 0.037355177,\n",
       " 0.03600418,\n",
       " 0.039628703,\n",
       " 0.04411427,\n",
       " 0.04062333,\n",
       " 0.034436114,\n",
       " 0.13706674,\n",
       " 0.19282241,\n",
       " 0.050232522,\n",
       " 0.05112658,\n",
       " 0.04244598,\n",
       " 0.34802967,\n",
       " 0.044045657,\n",
       " 0.09588582,\n",
       " 0.043799922,\n",
       " 0.093658574,\n",
       " 0.03528646,\n",
       " 0.038504913,\n",
       " 0.12698469,\n",
       " 0.056269836,\n",
       " 0.29100972,\n",
       " 0.03941994,\n",
       " 0.08325575,\n",
       " 0.3845618,\n",
       " 0.043904327,\n",
       " 0.041851033,\n",
       " 0.040197607,\n",
       " 0.1125211,\n",
       " 0.038916852,\n",
       " 0.39568123,\n",
       " 0.03627357,\n",
       " 0.044039182,\n",
       " 0.045288604,\n",
       " 0.043352615,\n",
       " 0.042024985,\n",
       " 0.03827517,\n",
       " 0.09879748,\n",
       " 0.044122107,\n",
       " 0.03579437,\n",
       " 0.043321032,\n",
       " 0.35455373,\n",
       " 0.049340587,\n",
       " 0.19200769,\n",
       " 0.1822851,\n",
       " 0.045544177,\n",
       " 0.042414807,\n",
       " 0.03748221,\n",
       " 0.039013926,\n",
       " 0.041557148,\n",
       " 0.12421955,\n",
       " 0.038854934,\n",
       " 0.05441541,\n",
       " 0.11651571,\n",
       " 0.07980529,\n",
       " 0.04269552,\n",
       " 0.067709096,\n",
       " 0.14473987,\n",
       " 0.042940293,\n",
       " 0.053708628,\n",
       " 0.06586167,\n",
       " 0.040794093,\n",
       " 0.038309157,\n",
       " 0.04399532,\n",
       " 0.12300789,\n",
       " 0.04404584,\n",
       " 0.11781427,\n",
       " 0.039374605,\n",
       " 0.045043286,\n",
       " 0.04477261,\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
