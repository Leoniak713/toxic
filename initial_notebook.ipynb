{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import transformers\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.optim import *\n",
    "from torch.nn.modules.loss import *\n",
    "from torch.optim.lr_scheduler import * \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Seeds basic parameters for reproductibility of results\n",
    "    \n",
    "    Arguments:\n",
    "        seed {int} -- Number of the seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2020\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Base class that defines common API for datasets.\"\"\"\n",
    "\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if self.y is not None:\n",
    "            return self.x[index], self.y[index]\n",
    "        return self.x[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tokenizer, nrows, max_len, data_cache_dir, overwrite):\n",
    "    loaded_cache = False\n",
    "    if os.path.exists(data_cache_dir):\n",
    "        if overwrite:\n",
    "            shutil.rmtree(data_cache_dir)\n",
    "        else:\n",
    "            x_train = np.load(os.path.join(data_cache_dir, \"x_train\"))\n",
    "            x_valid = np.load(os.path.join(data_cache_dir, \"x_valid\"))\n",
    "            x_test = np.load(os.path.join(data_cache_dir, \"x_test\"))\n",
    "            y_train = np.load(os.path.join(data_cache_dir, \"y_train\"))\n",
    "            y_valid = np.load(os.path.join(data_cache_dir, \"y_valid\"))\n",
    "            loaded_cache = True\n",
    "        \n",
    "    if not loaded_cache:\n",
    "        train1 = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\", nrows=nrows)\n",
    "        train2 = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\", nrows=nrows)\n",
    "        train2.toxic = train2.toxic.round().astype(int)\n",
    "\n",
    "        valid = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/validation.csv', nrows=nrows)\n",
    "        test = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/test.csv', nrows=nrows)\n",
    "        if nrows is None:\n",
    "            train = pd.concat([\n",
    "                train1[['comment_text', 'toxic']],\n",
    "                train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "                train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "            ])\n",
    "        else:\n",
    "            train = pd.concat([\n",
    "                train1[['comment_text', 'toxic']],\n",
    "                train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "                train2[['comment_text', 'toxic']].query('toxic==0')\n",
    "            ])\n",
    "\n",
    "        x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=max_len)\n",
    "        x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=max_len)\n",
    "        x_test = regular_encode(test.content.values, tokenizer, maxlen=max_len)\n",
    "\n",
    "        y_train = train.toxic.values\n",
    "        y_valid = valid.toxic.values\n",
    "        \n",
    "        os.makedirs(data_cache_dir)\n",
    "        \n",
    "        np.save(os.path.join(data_cache_dir, \"x_train\"), x_train)\n",
    "        np.save(os.path.join(data_cache_dir, \"x_valid\"), x_valid)\n",
    "        np.save(os.path.join(data_cache_dir, \"x_test\"), x_test)\n",
    "        np.save(os.path.join(data_cache_dir, \"y_train\"), y_train)\n",
    "        np.save(os.path.join(data_cache_dir, \"y_valid\"), y_valid)\n",
    "    \n",
    "    train_dataset = Dataset(x_train, y_train)\n",
    "    valid_dataset = Dataset(x_valid, y_valid)\n",
    "    test_dataset = Dataset(x_test)\n",
    "    \n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, transformer, num_classes=1):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \n",
    "        Arguments:\n",
    "            model {string} -- Transformer to build the model on. Expects \"camembert-base\".\n",
    "            num_classes {int} -- Number of classes (default: {1})\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.nb_features = self.transformer.pooler.dense.out_features\n",
    "\n",
    "        self.pooler = nn.Sequential(\n",
    "            nn.Linear(self.nb_features, self.nb_features), \n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.logit = nn.Linear(self.nb_features, num_classes)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        \"\"\"\n",
    "        Usual torch forward function\n",
    "        \n",
    "        Arguments:\n",
    "            tokens {torch tensor} -- Sentence tokens\n",
    "        \n",
    "        Returns:\n",
    "            torch tensor -- Class logits\n",
    "        \"\"\"\n",
    "        hidden_states, _ = self.transformer(\n",
    "            tokens, attention_mask=(tokens > 0).long()\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states[:, 0] # Use the representation of the first token of the last layer\n",
    "\n",
    "        ft = self.pooler(hidden_states)\n",
    "\n",
    "        return self.logit(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_dataset, val_dataset, epochs=1, batch_size=32, warmup_prop=0, lr=5e-5):\n",
    "    device = 'cuda' #xm.xla_device()\n",
    "    model.to(device)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    num_warmup_steps = int(warmup_prop * epochs * len(train_loader))\n",
    "    num_training_steps = epochs * len(train_loader)\n",
    "    \n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    loss_fct = nn.BCEWithLogitsLoss(reduction='mean').to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        avg_loss = 0\n",
    "        \n",
    "        for step, (x, y_batch) in tqdm(enumerate(train_loader), total=len(train_loader)): \n",
    "            y_pred = model(x.to(device))\n",
    "            \n",
    "            loss = loss_fct(y_pred.view(-1).float(), y_batch.float().to(device))\n",
    "            loss.backward()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            print('{step}/{total}'.format(step=step, total=len(train_loader)))\n",
    "                \n",
    "        model.eval()\n",
    "        preds = []\n",
    "        truths = []\n",
    "        avg_val_loss = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y_batch in val_loader:                \n",
    "                y_pred = model(x.to(device))\n",
    "                loss = loss_fct(y_pred.detach().view(-1).float(), y_batch.float().to(device))\n",
    "                avg_val_loss += loss.item() / len(val_loader)\n",
    "                \n",
    "                probs = torch.sigmoid(y_pred).detach().cpu().numpy()\n",
    "                preds += list(probs.flatten())\n",
    "                truths += list(y_batch.numpy().flatten())\n",
    "            score = roc_auc_score(truths, preds)\n",
    "            \n",
    "        \n",
    "        dt = time.time() - start_time\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        print('Epoch {epoch}/{epochs} \\t lr={lr} \\t t={dt}s \\t loss={avg_loss} \\t val_loss={avg_val_loss} \\t val_auc={score}'.format(\n",
    "            epoch=epoch,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            dt=dt,\n",
    "            avg_loss=avg_loss,\n",
    "            avg_val_loss=avg_val_loss,\n",
    "            score=score\n",
    "        ))\n",
    "        #print(f'Epoch {epoch + 1}/{epochs} \\t lr={lr:.1e} \\t t={dt:.0f}s \\t loss={avg_loss:.4f} \\t val_loss={avg_val_loss:.4f} \\t val_auc={score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "        train_dataset, \n",
    "        val_dataset, \n",
    "        model_cache_dir, \n",
    "        overwrite, \n",
    "        transformer_type, \n",
    "        epochs=1, \n",
    "        batch_size=32, \n",
    "        warmup_prop=0, \n",
    "        lr=5e-5\n",
    "    ):\n",
    "    classifier = Classifier(AutoModel.from_pretrained(transformer_type))\n",
    "    loaded_cache = False\n",
    "    if os.path.exists(model_cache_dir):\n",
    "        if overwrite:\n",
    "            shutil.rmtree(data_cache_dir)\n",
    "        else:\n",
    "            classifier.load_state_dict(model_cache_dir)\n",
    "            loaded_cache = True\n",
    "    if not loaded_cache:\n",
    "        fit(classifier, train_dataset, val_dataset, epochs=1, batch_size=32, warmup_prop=0, lr=5e-5)\n",
    "        os.makedirs(model_cache_dir)\n",
    "        classifier.save_state_dict(model_cache_dir)\n",
    "    return classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset, batch_size=16):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    truths = []\n",
    "    avg_val_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, x in tqdm(enumerate(loader), total=len(loader)):                \n",
    "            y_pred = model(x.to('cuda'))\n",
    "            probs = torch.sigmoid(y_pred).detach().cpu().numpy()\n",
    "            preds += list(probs.flatten())\n",
    "            \n",
    "    sub = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv', nrows=nrows)\n",
    "    sub['toxic'] = preds\n",
    "    sub.to_csv('submission.csv', index=False)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 10\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "warmup_prop = 0\n",
    "lr = 2e-5\n",
    "max_len = 192\n",
    "MODEL = 'xlm-roberta-base'\n",
    "device = 'cuda'\n",
    "data_cache_dir = 'data_cache'\n",
    "data_overwrite = True\n",
    "model_cache_dir = 'model_cache'\n",
    "model_overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = get_data(tokenizer, nrows, max_len, data_cache_dir, data_overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_model(\n",
    "    train_dataset, \n",
    "    valid_dataset, \n",
    "    model_cache_dir, \n",
    "    model_overwrite,\n",
    "    MODEL, \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size,\n",
    "    warmup_prop=warmup_prop, \n",
    "    lr=lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier(AutoModel.from_pretrained(MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model, dataset, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
